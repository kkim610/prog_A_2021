{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 21. Image Pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "일반적으로는 고정된 이미지 사이즈를 작업을 하지만, 때때로 동일한 이미지에 대해서 다양한 사이즈를 가지고 작업을 해야 하는 경우가 있습니다. 만일, 이미지에서 얼굴을 찾을 경우에 얼굴의 사이즈를 확신할 수 없습니다. 이럴 경우에는 원본 이미지에 대한 다양한 사이즈에서 얼굴을 찾는다면 좀더 정확하고 확실한 이미지를 찾을 수 있습니다. 이 처럼 동일 이미지의 서로 다른 사이즈의 set을 Image Pyramids라고 합니다(가장 아래에 가장 큰 해상도를 놓고 점점 줄여가면서 쌓아가는 형태입니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Pyramid, or pyramid representation, is a type of multi-scale signal representation developed by the computer vision, image processing and signal processing communities, in which a signal or an image is subject to repeated smoothing and subsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/fig_05.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "opencv에는 두 종류의 Pyramid\n",
    "1. Gaussian Pyramid\n",
    "2. Laplacian Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gaussian Pyramid 의 pyrDown 필터 (M x N 사이지의 이미지는 M/2 X N/2 가 적용되어 크기가  1/4사이즈로 줄어들게 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"pic/lena.jpg\")\n",
    "lr = cv2.pyrDown(img)\n",
    "\n",
    "cv2.imshow(\"Original image\", img)\n",
    "cv2.imshow(\"pyrDown 1 image\", lr)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"pic/lena.jpg\")\n",
    "lr1 = cv2.pyrDown(img)\n",
    "lr2 = cv2.pyrDown(lr1)\n",
    "\n",
    "cv2.imshow(\"Original image\", img)\n",
    "cv2.imshow(\"pyrDown 1 image\", lr1)\n",
    "cv2.imshow(\"pyrDown 2 image\", lr2)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "pyrUP 을 사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"pic/lena.jpg\")\n",
    "lr1 = cv2.pyrDown(img)\n",
    "lr2 = cv2.pyrDown(lr1)\n",
    "hr1 = cv2.pyrUp(lr2)\n",
    "\n",
    "cv2.imshow(\"Original image\", img)\n",
    "cv2.imshow(\"pyrDown 1 image\", lr1)\n",
    "cv2.imshow(\"pyrDown 2 image\", lr2)\n",
    "cv2.imshow(\"pyrUp 1 image\", hr1)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "lr1 과 hr1을 비교해보면 동일하지 않다.  \n",
    "lr2로 pyrDown될때 lr2의 Information 일부를 잃어버렸다.  \n",
    "따라서, pyrUp 해도 잃어버린 Information을 복구할 수는 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "반복문을 사용하여 여러레벨의 이미지를 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"pic/lena.jpg\")\n",
    "layer = img.copy()\n",
    "gaussian_pyramid_list = [layer]\n",
    "\n",
    "for i in range(3):\n",
    "    layer = cv2.pyrDown(layer)\n",
    "    gaussian_pyramid_list.append(layer)\n",
    "    cv2.imshow(str(i), layer)\n",
    "\n",
    "cv2.imshow(\"Original image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Laplacian Pyramid는 Gaussian Pyramid에서 만들어 집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "Laplacian Pyramid image는   \n",
    "1) 어떤 레벨의 image와   \n",
    "2) 한 단계 낮은 resolution의 image에 pyrUp을 적용한 image  \n",
    "와의 차이를 나타낸다.\n",
    "\n",
    "이렇게 하면 외곽선이 남게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"pic/lena.jpg\")\n",
    "layer = img.copy()\n",
    "gaussian_pyramid_list = [layer]\n",
    "\n",
    "for i in range(6):\n",
    "    layer = cv2.pyrDown(layer)\n",
    "    gaussian_pyramid_list.append(layer)\n",
    "    #cv2.imshow(str(i), layer)\n",
    "\n",
    "layer = gaussian_pyramid_list[5]\n",
    "cv2.imshow('upper level Gaussian Pyramid', layer)\n",
    "laplacian_pyramid_list = [layer]\n",
    "\n",
    "for i in range(5, 0, -1):\n",
    "    gaussian_extended = cv2.pyrUp(gaussian_pyramid_list[i])\n",
    "    laplacian = cv2.subtract(gaussian_pyramid_list[i-1], gaussian_extended)\n",
    "    cv2.imshow(str(i), laplacian)\n",
    "\n",
    "cv2.imshow(\"Original image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 22. Image Blending using Pyramids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "apple = cv2.imread('pic/apple.jpg')\n",
    "orange = cv2.imread('pic/orange.jpg')\n",
    "print(apple.shape)\n",
    "print(orange.shape)\n",
    "\n",
    "cv2.imshow(\"apple\", apple)\n",
    "cv2.imshow(\"orange\", orange)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "두 그림의 사이즈가 같다.\n",
    "사과 왼쪽 반절과 오렌지 오른쪽 반절을 합성하자.  \n",
    "그냥 오려 붙이기를 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "apple = cv2.imread('pic/apple.jpg')\n",
    "orange = cv2.imread('pic/orange.jpg')\n",
    "#print(apple.shape)\n",
    "#print(orange.shape)\n",
    "apple_orange = np.hstack((apple[:, :256], orange[:, 256:]))\n",
    "\n",
    "cv2.imshow(\"apple\", apple)\n",
    "cv2.imshow(\"orange\", orange)\n",
    "cv2.imshow(\"apple_orange\", apple_orange)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "image blending 절차\n",
    "1. Load thr two images of apple and orang.\n",
    "2. Find the Gaussian Pyramids for apple and orange (in this example, number of level is 6)\n",
    "3. From Gaussian Pyramids, find their Laplacian Pyramids\n",
    "4. Now join the left half of apple and right half of orange in each level of Laplacian Pyramids.\n",
    "5. Fianlly from this joint image pyramids, reconstruct the originalimage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "apple = cv2.imread('pic/apple.jpg')\n",
    "orange = cv2.imread('pic/orange.jpg')\n",
    "#print(apple.shape)\n",
    "#print(orange.shape)\n",
    "apple_orange = np.hstack((apple[:, :256], orange[:, 256:]))\n",
    "\n",
    "# generate Gaussian pyramid for apple\n",
    "apple_copy = apple.copy()\n",
    "gp_apple = [apple_copy]\n",
    "for i in range(6):\n",
    "    apple_copy = cv2.pyrDown(apple_copy)\n",
    "    gp_apple.append(apple_copy)\n",
    "\n",
    "# generate Gaussian pyramid for orange\n",
    "orange_copy = orange.copy()\n",
    "gp_orange = [orange_copy]\n",
    "for i in range(6):\n",
    "    orange_copy = cv2.pyrDown(orange_copy)\n",
    "    gp_orange.append(orange_copy)\n",
    "    \n",
    "# generate Laplacian Pyramid for apple\n",
    "apple_copy = gp_apple[5]\n",
    "lp_apple = [apple_copy]\n",
    "for i in range(5, 0, -1):\n",
    "    gaussian_expanded = cv2.pyrUp(gp_apple[i])\n",
    "    laplacian = cv2.subtract(gp_apple[i-1], gaussian_expanded)\n",
    "    lp_apple.append(laplacian)\n",
    "    \n",
    "# generate Laplacian Pyramid for orange\n",
    "orange_copy = gp_orange[5]\n",
    "lp_orange = [orange_copy]\n",
    "for i in range(5, 0, -1):\n",
    "    gaussian_expanded = cv2.pyrUp(gp_orange[i])\n",
    "    laplacian = cv2.subtract(gp_orange[i-1], gaussian_expanded)\n",
    "    lp_orange.append(laplacian)\n",
    "\n",
    "# Now add left and right halves of images in each level\n",
    "apple_orange_pyramid = []\n",
    "n = 0\n",
    "for apple_lap, orange_lap in zip(lp_apple, lp_orange):\n",
    "    n += 1\n",
    "    cols, rows, ch = apple_lap.shape\n",
    "    laplacian = np.hstack((apple_lap[:, 0:int(cols/2)], orange_lap[:, int(cols/2):]))\n",
    "    apple_orange_pyramid.append(laplacian)\n",
    "\n",
    "# now reconstruct\n",
    "apple_orange_reconstruct = apple_orange_pyramid[0]\n",
    "for i in range(1, 6):\n",
    "    apple_orange_reconstruct = cv2.pyrUp(apple_orange_reconstruct)\n",
    "    apple_orange_reconstruct = cv2.add(apple_orange_pyramid[i], apple_orange_reconstruct)\n",
    "\n",
    "cv2.imshow(\"apple\", apple)\n",
    "cv2.imshow(\"orange\", orange)\n",
    "cv2.imshow(\"apple_orange\", apple_orange)\n",
    "cv2.imshow(\"apple_orange_reconstruct\", apple_orange_reconstruct)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 23. Find and Draw Contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Contours란 동일한 색 또는 동일한 강도를 가지고 있는 영역의 경계선을 연결한 선입니다.   \n",
    "우리가 자주 보는 것으로는 등고선이나 일기예보에서 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "대상의 외형을 파악하는데 유용하게 사용이 됩니다.\n",
    "\n",
    "- 정확도를 높히기 위해서 Binary Image를 사용합니다. \n",
    "- threshold나 canny edge를 선처리로 수행합니다.\n",
    "- cv2.findContours() 함수는 원본 이미지를 직접 수정하기 때문에, 원본 이미지를 보존 하려면 Copy해서 사용해야 합니다.\n",
    "- OpenCV에서는 contours를 찾는 것은 검은색 배경에서 하얀색 대상을 찾는 것과 비슷합니다. 그래서 대상은 흰색, 배경은 검은색으로 해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### OpenCV에서 contours를 찾을 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "cv2.findContours(image, mode, method[, contours[, hierarchy[, offset]]]) → image, contours, hierarchy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "Parameters:\n",
    "\n",
    "image – 8-bit single-channel image. binary image.\n",
    "\n",
    "mode – contours를 찾는 방법\n",
    "    cv2.RETR_EXTERNAL : contours line중 가장 바같쪽 Line만 찾음.\n",
    "    cv2.RETR_LIST : 모든 contours line을 찾지만, hierachy 관계를 구성하지 않음.\n",
    "    cv2.RETR_CCOMP : 모든 contours line을 찾으며, hieracy관계는 2-level로 구성함.\n",
    "    cv2.RETR_TREE : 모든 contours line을 찾으며, 모든 hieracy관계를 구성함.\n",
    "\n",
    "method – contours를 찾을 때 사용하는 근사치 방법\n",
    "    cv2.CHAIN_APPROX_NONE : 모든 contours point를 저장.\n",
    "    cv2.CHAIN_APPROX_SIMPLE : contours line을 그릴 수 있는 point 만 저장. (ex; 사각형이면 4개 point)\n",
    "    cv2.CHAIN_APPROX_TC89_L1 : contours point를 찾는 algorithm\n",
    "    cv2.CHAIN_APPROX_TC89_KCOS : contours point를 찾는 algorithm\n",
    "\n",
    "Returns: image, contours , hierachy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Method에 대해서 설명을 하면 사각형의 contours line을 그릴 때, cv2.CHAIN_APPROX_NONE 는 모든 point를 저장하고 cv2.CHAIN_APPROX_SIMPLE 는 4개의 point만을 저장하여 메모리를 절약합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "contours[0].shape #cv2.CHAIN_APPROX_SIMPLE(4 point)\n",
    "(4, 1, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "contours[0].shape #CHAIN_APPROX_NONE(1308 point)\n",
    "(1308, 1, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/shapes5.jpg')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('Image', img)\n",
    "cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/shapes5.jpg')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#ret, thresh = cv2.threshold(imgray, 50, 255, 0)\n",
    "#contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "imgGrey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "_, thrash = cv2.threshold(imgGrey, 240, 255, cv2.THRESH_BINARY) #240\n",
    "contours, _ = cv2.findContours(thrash, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# Contours is a python list of all the contours in the image\n",
    "# Each individual contour is a numpy array of (x,y) coordinates of boundary points of the objects.\n",
    "\n",
    "print(\"Number of contours = \" + str(len(contours)))\n",
    "print(contours[0].shape)\n",
    "#print(contours)\n",
    "#cv2.imshow('Image', img)\n",
    "#cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/shapes5.jpg')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(imgray, 240, 255, 0)\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Contours is a python list of all the contours in the image\n",
    "# Each individual contour is a numpy array of (x,y) coordinates of boundary points of the objects.\n",
    "\n",
    "print(\"Number of contours = \" + str(len(contours)))\n",
    "print(contours[0].shape)\n",
    "print(contours)\n",
    "#cv2.imshow('Image', img)\n",
    "#cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/opencv-logo.png')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('Image', img)\n",
    "cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/opencv-logo.png')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(imgray, 127, 255, 0)\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# Contours is a python list of all the contours in the image\n",
    "# Each individual contour is a numpy array of (x,y) coordinates of boundary points of the objects.\n",
    "\n",
    "print(\"Number of contours = \" + str(len(contours)))\n",
    "#print(contours[0].shape)\n",
    "#cv2.imshow('Image', img)\n",
    "#cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### OpenCV에서 contours를 그릴 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "cv2.drawContours(image, contours, contourIdx, color[, thickness[, lineType[, hierarchy[, maxLevel[, offset]]]]]) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "Parameters:\t\n",
    "    image – 원본 이미지\n",
    "    contours – contours정보.\n",
    "    contourIdx – contours list type에서 몇번째 contours line을 그릴 것인지. -1 이면 전체\n",
    "    color – contours line color\n",
    "    thickness – contours line의 두께. 음수이면 contours line의 내부를 채움.\n",
    "\n",
    "Returns: image에 contours가 그려진 결과\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/shapes5.jpg')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#ret, thresh = cv2.threshold(imgray, 0, 255, 0)\n",
    "#contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "imgGrey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "_, thresh = cv2.threshold(imgGrey, 240, 255, cv2.THRESH_BINARY) #240\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "for contour in contours[1:]:\n",
    "    approx = cv2.approxPolyDP(contour, 0.01* cv2.arcLength(contour, True), True)\n",
    "    cv2.drawContours(img, [approx], 0, (0, 0, 255), 5)\n",
    "\n",
    "\n",
    "# Contours is a python list of all the contours in the image\n",
    "# Each individual contour is a numpy array of (x,y) coordinates of boundary points of the objects.\n",
    "\n",
    "print(\"Number of contours = \" + str(len(contours)))\n",
    "print(contours[0].shape)\n",
    "#print(contours)\n",
    "cv2.imshow('Image', img)\n",
    "#cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/shapes6.jpg')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#ret, thresh = cv2.threshold(imgray, 240, 255, 0)\n",
    "_, thresh = cv2.threshold(imgray, 240, 255, cv2.THRESH_BINARY) \n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "print(\"Number of contours = \" + str(len(contours)))\n",
    "#print(contours[0])\n",
    "\n",
    "cv2.drawContours(img, contours, -1, (0, 255, 0), 3) # -1 is contour index indicating all contours\n",
    "\n",
    "\n",
    "cv2.imshow('Image', img)\n",
    "#cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/opencv-logo.png')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#ret, thresh = cv2.threshold(imgray, 127, 255, 0)\n",
    "_, thresh = cv2.threshold(imgray, 200, 255, cv2.THRESH_BINARY) \n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "print(\"Number of contours = \" + str(len(contours)))\n",
    "#print(contours[0])\n",
    "\n",
    "cv2.drawContours(img, contours, -1, (0, 255, 255), 3) # -1 is contour index indicating all contours\n",
    "\n",
    "\n",
    "cv2.imshow('Image', img)\n",
    "#cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "contour index 0을 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/opencv-logo.png')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#ret, thresh = cv2.threshold(imgray, 200, 255, 0)\n",
    "_, thresh = cv2.threshold(imgray, 200, 255, cv2.THRESH_BINARY) \n",
    "\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "print(\"Number of contours = \" + str(len(contours)))\n",
    "print(contours[0])\n",
    "\n",
    "cv2.drawContours(img, contours, 0, (255, 0, 255), 3) # -1 is contour index indicating all contours\n",
    "\n",
    "\n",
    "cv2.imshow('Image', img)\n",
    "#cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/opencv-logo.png')\n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#ret, thresh = cv2.threshold(imgray, 200, 255, 0)\n",
    "_, thresh = cv2.threshold(imgray, 200, 255, cv2.THRESH_BINARY) \n",
    "\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "print(\"Number of contours = \" + str(len(contours)))\n",
    "print(contours[0])\n",
    "\n",
    "cv2.drawContours(img, contours, -1, (255, 255, 0), 3)\n",
    "cv2.drawContours(imgray, contours, -1, (0, 0, 0), 3)\n",
    "\n",
    "cv2.imshow('Image', img)\n",
    "cv2.imshow('Image GRAY', imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 24. Motion Detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('pic/vtest.avi')\n",
    "\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    cv2.imshow(\"inter\", frame)\n",
    "\n",
    "    if cv2.waitKey(40) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "움직이는 object에 사각형을 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('pic/vtest.avi')\n",
    "frame_width = int( cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "frame_height =int( cap.get( cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "\n",
    "out = cv2.VideoWriter(\"output.avi\", fourcc, 5.0, (1280,720))\n",
    "\n",
    "ret, frame1 = cap.read()\n",
    "ret, frame2 = cap.read()\n",
    "print(frame1.shape)\n",
    "\n",
    "while cap.isOpened():\n",
    "    diff = cv2.absdiff(frame1, frame2)\n",
    "    gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)\n",
    "    dilated = cv2.dilate(thresh, None, iterations=3)\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    cv2.drawContours(frame1, contours, -1, (0,255,0),2) # 2: thicjness\n",
    "    \n",
    "    cv2.imshow(\"feed\", frame1)\n",
    "    frame1 = frame2\n",
    "    ret, frame2 = cap.read()\n",
    "\n",
    "    if cv2.waitKey(40) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Noise를 제거하고, 사각형을 그리자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('pic/vtest.avi')\n",
    "frame_width = int( cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "frame_height =int( cap.get( cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "\n",
    "out = cv2.VideoWriter(\"output.avi\", fourcc, 5.0, (1280,720))\n",
    "\n",
    "ret, frame1 = cap.read()\n",
    "ret, frame2 = cap.read()\n",
    "print(frame1.shape)\n",
    "\n",
    "while cap.isOpened():\n",
    "    diff = cv2.absdiff(frame1, frame2)\n",
    "    gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)\n",
    "    dilated = cv2.dilate(thresh, None, iterations=3)\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for contour in contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "\n",
    "        if cv2.contourArea(contour) < 700:\n",
    "            continue\n",
    "        cv2.rectangle(frame1, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame1, \"Status: {}\".format('Movement'), (10, 20), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1, (0, 0, 255), 3)\n",
    "        \n",
    "    #cv2.drawContours(frame1, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"feed\", frame1)\n",
    "    frame1 = frame2\n",
    "    ret, frame2 = cap.read()\n",
    "\n",
    "    if cv2.waitKey(40) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "contourArea(contour) < 900 으로 바꾸자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('pic/vtest.avi')\n",
    "frame_width = int( cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "frame_height =int( cap.get( cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "\n",
    "out = cv2.VideoWriter(\"output.avi\", fourcc, 5.0, (1280,720))\n",
    "\n",
    "ret, frame1 = cap.read()\n",
    "ret, frame2 = cap.read()\n",
    "print(frame1.shape)\n",
    "\n",
    "while cap.isOpened():\n",
    "    diff = cv2.absdiff(frame1, frame2)\n",
    "    gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)\n",
    "    dilated = cv2.dilate(thresh, None, iterations=3)\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for contour in contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "\n",
    "        if cv2.contourArea(contour) < 900:\n",
    "            continue\n",
    "        cv2.rectangle(frame1, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame1, \"Status: {}\".format('Movement'), (10, 20), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1, (0, 0, 255), 3)\n",
    "        \n",
    "    #cv2.drawContours(frame1, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "    image = cv2.resize(frame1, (1280,720))\n",
    "    out.write(image)\n",
    "    cv2.imshow(\"feed\", frame1)\n",
    "    frame1 = frame2\n",
    "    ret, frame2 = cap.read()\n",
    "\n",
    "    if cv2.waitKey(40) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 물체 따라가기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 과제: 두개 이상의 물체 따라가기 \n",
    "## 힌트: 9장 참고\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 25. Detect Simple Geometric Shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/shapes6.jpg')\n",
    "\n",
    "cv2.imshow(\"shapes\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('pic/shapes6.jpg')\n",
    "imgGrey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "_, thrash = cv2.threshold(imgGrey, 240, 255, cv2.THRESH_BINARY)\n",
    "contours, _ = cv2.findContours(thrash, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "for contour in contours:\n",
    "    approx = cv2.approxPolyDP(contour, 0.01* cv2.arcLength(contour, True), True)\n",
    "    cv2.drawContours(img, [approx], 0, (0, 0, 0), 5)\n",
    "    x = approx.ravel()[0]\n",
    "    y = approx.ravel()[1] - 5\n",
    "    if len(approx) == 3:\n",
    "        cv2.putText(img, \"Triangle\", (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))\n",
    "    elif len(approx) == 4:\n",
    "        x1 ,y1, w, h = cv2.boundingRect(approx)\n",
    "        aspectRatio = float(w)/h\n",
    "        print(aspectRatio)\n",
    "        if aspectRatio >= 0.95 and aspectRatio <= 1.05:\n",
    "          cv2.putText(img, \"square\", (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))\n",
    "        else:\n",
    "          cv2.putText(img, \"rectangle\", (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))\n",
    "    elif len(approx) == 5:\n",
    "        cv2.putText(img, \"Pentagon\", (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))\n",
    "    elif len(approx) == 10:\n",
    "        cv2.putText(img, \"Star\", (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))\n",
    "    else:\n",
    "        cv2.putText(img, \"Circle\", (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))\n",
    "\n",
    "\n",
    "cv2.imshow(\"shapes\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 2.6 Understanding image Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = np.zeros((200,200), np.uint8)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = np.zeros((200,200), np.uint8)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "\n",
    "plt.hist(img.ravel(), 256, [0, 256])\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = np.zeros((200,200), np.uint8)\n",
    "cv2.rectangle(img, (0, 100), (200, 200), (255), -1)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "\n",
    "\n",
    "plt.hist(img.ravel(), 256, [0, 256])\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = np.zeros((200,200), np.uint8)\n",
    "cv2.rectangle(img, (0, 100), (200, 200), (255), -1)\n",
    "cv2.rectangle(img, (0, 50), (100, 100), (127), -1)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "\n",
    "\n",
    "plt.hist(img.ravel(), 256, [0, 256])\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"pic/lena.jpg\",0)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "\n",
    "\n",
    "plt.hist(img.ravel(), 256, [0, 256])\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"pic/lena.jpg\")\n",
    "\n",
    "b, g, r = cv2.split(img)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "cv2.imshow(\"b\", b)\n",
    "cv2.imshow(\"g\", g)\n",
    "cv2.imshow(\"r\", r)\n",
    "\n",
    "plt.hist(b.ravel(), 256, [0, 256])\n",
    "plt.hist(g.ravel(), 256, [0, 256])\n",
    "plt.hist(r.ravel(), 256, [0, 256])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"pic/lena.jpg\",0)\n",
    "\n",
    "hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n",
    "plt.plot(hist)\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"pic/lena.jpg\")\n",
    "\n",
    "hist = cv2.calcHist([img], [0], None, [256], [0, 256]) #blue\n",
    "plt.plot(hist)\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 27. Template matching "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"pic/messi5.jpg\")\n",
    "template = cv2.imread(\"pic/messi_face.jpg\")\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "cv2.imshow(\"template\", template)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"pic/messi5.jpg\")\n",
    "grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "template = cv2.imread(\"pic/messi_face.jpg\", 0)\n",
    "\n",
    "res = cv2.matchTemplate(grey_img, template, cv2.TM_CCORR_NORMED )\n",
    "print(res)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"pic/messi5.jpg\")\n",
    "grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "template = cv2.imread(\"pic/messi_face.jpg\", 0)\n",
    "\n",
    "w, h = template.shape[::-1]\n",
    "\n",
    "res = cv2.matchTemplate(grey_img, template, cv2.TM_CCORR_NORMED )\n",
    "print(\"res\",res)\n",
    "print(\"img\",img.shape)\n",
    "print(\"template\",template.shape)\n",
    "print(\"res\",res.shape)\n",
    "\n",
    "threshold = 0.8;\n",
    "loc = np.where(res >= threshold)\n",
    "print(loc)\n",
    "\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"pic/messi5.jpg\")\n",
    "grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "template = cv2.imread(\"pic/messi_face.jpg\", 0)\n",
    "\n",
    "w, h = template.shape[::-1]\n",
    "\n",
    "res = cv2.matchTemplate(grey_img, template, cv2.TM_CCORR_NORMED )\n",
    "print(res)\n",
    "\n",
    "threshold = 0.97;\n",
    "loc = np.where(res >= threshold)\n",
    "print(loc)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 28. Hough Line Transform Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The Hough transform is a feature extraction technique used in image analysis, computer vision, and digital image processing.  \n",
    "The purpose of the technique is to find imperfect instances of objects within a certain class of shapes by a voting procedure.   \n",
    "This voting procedure is carried out in a parameter space, from which object candidates are obtained as local maxima in a so-called accumulator space that is explicitly constructed by the algorithm for computing the Hough transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 이론: 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Hough Transform Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "1. Edge detection, e.g. using the Canny edge detector.\n",
    "2. Mapping of edge points to the Hough space and storage in an accumulator.\n",
    "3. Interpretation of the accumulator to yield lines of infinite length. The interpretation is done by thresholding and possibly other constraints.\n",
    "4. Conversion of infinite lines to finite lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "OpenCV implements two kinds of Hough Line Transforms:\n",
    "* Standard Hough Transform (HoughLines method)\n",
    "* Probabilistic Houfh Transform (HoughLinesP method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 29. Hough Line Transform using HoughLines method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('pic/sudoku.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "k = cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('pic/sudoku.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "cv2.imshow('edges', edges)\n",
    "lines = cv2.HoughLines(edges, 1, np.pi / 180, 200) # result is a list of polar coordinates.\n",
    "\n",
    "for line in lines:\n",
    "    rho,theta = line[0]\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a * rho # x coordinate of top left corner\n",
    "    y0 = b * rho # y coordinate of top left corner\n",
    "    # x1 stores the rounded off value of (r * cos(theta) - 1000 * sin(theta))\n",
    "    x1 = int(x0 + 1000 * (-b))\n",
    "    # y1 stores the rounded off value of (r * sin(theta)+ 1000 * cos(theta))\n",
    "    y1 = int(y0 + 1000 * (a))\n",
    "    # x2 stores the rounded off value of (r * cos(theta)+ 1000 * sin(theta))\n",
    "    x2 = int(x0 - 1000 * (-b))\n",
    "    # y2 stores the rounded off value of (r * sin(theta)- 1000 * cos(theta))\n",
    "    y2 = int(y0 - 1000 * (a))\n",
    "    cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "k = cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "1000 을 하는 이유:  https://stackoverflow.com/questions/18782873/houghlines-transform-in-opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "즉 1000 단위 만큼 양쪽으로 떨어진 두 점을 구하기 위함. 따라서 다른 숫자도 가능 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "cv2.HoughLines 의 문제점: 선분이 아니라 직선으로 나타난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 30. Probabilistic Hough Transform using HoughLinesP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('pic/sudoku.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray,50,150,apertureSize = 3)\n",
    "cv2.imshow('edges', edges)\n",
    "\n",
    "lines = cv2.HoughLinesP(edges,1,np.pi/180,100,minLineLength=100,maxLineGap=10)\n",
    "\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "k = cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('pic/road.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray,50,150,apertureSize = 3)\n",
    "cv2.imshow('edges', edges)\n",
    "\n",
    "lines = cv2.HoughLinesP(edges,1,np.pi/180,100,minLineLength=100,maxLineGap=10)\n",
    "\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "k = cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 31. Road Lane Line Detection (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "\n",
    "image = cv2.imread('pic/road.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "\n",
    "image = cv2.imread('pic/road.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(image.shape)\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "\n",
    "image = cv2.imread('pic/road.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(image.shape)\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "\n",
    "region_of_interest_vertices = [\n",
    "    (0, height),\n",
    "    (width/2, height/2),\n",
    "    (width, height)\n",
    "]\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    channel_count = img.shape[2]\n",
    "    match_mask_color = (255,) * channel_count\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "cropped_image = region_of_interest(image,\n",
    "                np.array([region_of_interest_vertices], np.int32),)\n",
    "\n",
    "plt.imshow(cropped_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 32. Road Lane Line Detection (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    channel_count = img.shape[2]\n",
    "    match_mask_color = (255,) * channel_count\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "image = cv2.imread('pic/road.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(image.shape)\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "\n",
    "region_of_interest_vertices = [\n",
    "    (0, height),\n",
    "    (width/2, height/2),\n",
    "    (width, height)\n",
    "]\n",
    "\n",
    "cropped_image = region_of_interest(image,\n",
    "                np.array([region_of_interest_vertices], np.int32),)\n",
    "\n",
    "gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_RGB2GRAY)\n",
    "canny_image = cv2.Canny(gray_image, 100, 200)\n",
    "\n",
    "\n",
    "plt.imshow(canny_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "region of interest를 사용하니, 차선이 잘 인식 되었다.  \n",
    "다음으로, region of interest 를 나타내는 삼각형의 외곽선을 제거하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    #channel_count = img.shape[2]\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "image = cv2.imread('pic/road.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "\n",
    "region_of_interest_vertices = [\n",
    "    (0, height),\n",
    "    (width/2, height/2),\n",
    "    (width, height)\n",
    "]\n",
    "\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "canny_image = cv2.Canny(gray_image, 100, 200)\n",
    "cropped_image = region_of_interest(canny_image,\n",
    "                np.array([region_of_interest_vertices], np.int32),)\n",
    "\n",
    "plt.imshow(cropped_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    #channel_count = img.shape[2]\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def drow_the_lines(img, lines):\n",
    "    img = np.copy(img)\n",
    "    blank_image = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            cv2.line(blank_image, (x1,y1), (x2,y2), (0, 255, 0), thickness=2)\n",
    "\n",
    "    img = cv2.addWeighted(img, 0.8, blank_image, 1, 0.0)\n",
    "    return img\n",
    "\n",
    "image = cv2.imread('pic/road.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "region_of_interest_vertices = [\n",
    "    (0, height),\n",
    "    (width/2, height/2),\n",
    "    (width, height)\n",
    "]\n",
    "\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "canny_image = cv2.Canny(gray_image, 100, 200)\n",
    "cropped_image = region_of_interest(canny_image,\n",
    "                np.array([region_of_interest_vertices], np.int32),)\n",
    "\n",
    "lines = cv2.HoughLinesP(cropped_image,\n",
    "                        rho=6,\n",
    "                        theta=np.pi/180,\n",
    "                        threshold=160,\n",
    "                        lines=np.array([]),\n",
    "                        minLineLength=40,\n",
    "                        maxLineGap=25)\n",
    "\n",
    "image_with_lines = drow_the_lines(image, lines)\n",
    "\n",
    "plt.imshow(image_with_lines)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 33. Road Lane Line Detection(Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "for video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    #channel_count = img.shape[2]\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def drow_the_lines(img, lines):\n",
    "    img = np.copy(img)\n",
    "    blank_image = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            cv2.line(blank_image, (x1,y1), (x2,y2), (0, 255, 0), thickness=10)\n",
    "\n",
    "    img = cv2.addWeighted(img, 0.8, blank_image, 1, 0.0)\n",
    "    return img\n",
    "\n",
    "# = cv2.imread('road.jpg')\n",
    "#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "def process(image):\n",
    "    #print(image.shape)\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    region_of_interest_vertices = [\n",
    "        (0, height),\n",
    "        (width/2, height/2),\n",
    "        (width, height)\n",
    "    ]\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    canny_image = cv2.Canny(gray_image, 100, 120)\n",
    "    cropped_image = region_of_interest(canny_image,\n",
    "                    np.array([region_of_interest_vertices], np.int32),)\n",
    "    lines = cv2.HoughLinesP(cropped_image,\n",
    "                            rho=2,\n",
    "                            theta=np.pi/180,\n",
    "                            threshold=50,\n",
    "                            lines=np.array([]),\n",
    "                            minLineLength=40,\n",
    "                            maxLineGap=100)\n",
    "    image_with_lines = drow_the_lines(image, lines)\n",
    "    return image_with_lines\n",
    "\n",
    "cap = cv2.VideoCapture('pic/test.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = process(frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
