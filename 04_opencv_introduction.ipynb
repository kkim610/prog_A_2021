{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 34. Circle Detection using Hough Circle Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('pic/smarties.png')\n",
    "output = img.copy()\n",
    "\n",
    "cv.imshow('output',output)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "circle equation: $(x - x_{center})^2 + (y - y_{center})^2 = r^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('pic/smarties.png')\n",
    "output = img.copy()\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "gray = cv.medianBlur(gray, 5)\n",
    "circles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, 20,\n",
    "                          param1=50, param2=30, minRadius=0, maxRadius=0)\n",
    "detected_circles = np.uint16(np.around(circles))\n",
    "for (x, y ,r) in detected_circles[0, :]:\n",
    "    cv.circle(output, (x, y), r, (100, 100, 100), 3)\n",
    "    cv.circle(output, (x, y), 2, (0, 255, 255), 3) #circle center 표시\n",
    "\n",
    "\n",
    "cv.imshow('output',output)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "cv2.HoughCircles(image, method, dp, minDist[, circles[, param1[, param2[, minRadius[, maxRadius]]]]]) → circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Parameters  \n",
    "* image: 입력 이미지는 그레이스케일 이미지여야 한다. \n",
    "* method: 원을 검출하는 방법, 현재는 HOUGH_GRADIENT만 사용할 수 있다. \n",
    "* dp: 이미지 해상도에 대한  accumulator 해상도의 비율의 역수. 예를 들어 dp=1이면 배열 accumulator는 입력 이미지와 같은 해상도를 가진다. dp=2이면 배열 accumulator는 절반의 너비와 높이를 가지게 된다. \n",
    "* minDist: 검출된 원사이의 최소 거리. 이 값이 너무 크면 검출되지 못한 원들이 생기며, 너무 작으면  인접한 원들이 잘못 검출될 수 있다.\n",
    "* param1: 지정한 원 검출방법을 위한 파라미터. HOUGH_GRADIENT의 경우  캐니 에지 디텍터의 높은 쓰레쏠드값. 낮은 쓰레쏠드값은 0.5배 해서 사용한다. \n",
    "* param2: 지정한 원 검출방법을 위한 파라미터. HOUGH_GRADIENT의 경우 accumulator 쓰레쏠드 값. 이 값이 너무 작으면 거짓 원이 검출된다. 가장 큰 accumulator 값을 가지는 원이 먼저 리턴된다. \n",
    "* minRadius: 검출하려는 원의 최소 반지름. 크기를 알 수 없는 경우 0으로 지정.  \n",
    "* maxRadius: 검출하려는 원의 최대 반지름. 크기를 알 수 없는 경우 0으로 지정.  음수를 지정하면 원의 중심만 리턴합니다. \n",
    "\n",
    "return  \n",
    "* circles: 발견한 원에 대한 벡터. 각 벡터는 3개 (x,y,radius)  또는 4개 (x,y,radius,votes) 의 원소를 가집니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('pic/shapes6.jpg')\n",
    "output = img.copy()\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "gray = cv.medianBlur(gray, 5)\n",
    "circles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, 20,\n",
    "                          param1=50, param2=30, minRadius=0, maxRadius=0)\n",
    "detected_circles = np.uint16(np.around(circles))\n",
    "for (x, y, r) in detected_circles[0, :]:\n",
    "    cv.circle(output, (x, y), r, (0, 255, 0), 3)\n",
    "    cv.circle(output, (x, y), 2, (0, 255, 255), 3) #circle center 표시 yellow\n",
    "\n",
    "\n",
    "cv.imshow('output',output)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 35. Face Detection using Haar Cascade Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Trained Classifiers: https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "\n",
    "haarcascade_frontalface_default.xml 이미 수업폴더에 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Cascading is a particular case of ensemble learning based on the concatenation of several classifiers, using all information collected from the output from a given classifier as additional information for the next classifier in the cascade. Unlike voting or stacking ensembles, which are multiexpert systems, cascading is a multistage one.\n",
    "\n",
    "Cascading classifiers are trained with several hundred \"positive\" sample views of a particular object and arbitrary \"negative\" images of the same size. After the classifier is trained it can be applied to a region of an image and detect the object in question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Cascade classifiers are trained on a few hundred sample images of image that contain the object we want to detect, and other images that do not contain those images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('pic/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Read the input image\n",
    "img = cv2.imread('pic/bong.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "for (x, y , w ,h) in faces:\n",
    "    cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)\n",
    "\n",
    "# Display the output\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('pic/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Read the input image\n",
    "img = cv2.imread('pic/son2.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "for (x, y , w ,h) in faces:\n",
    "    cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)\n",
    "\n",
    "# Display the output\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "* step 0: download haarcascade_frontalface_default.xml to your local computer. \n",
    "* step 1: (line 3) declare classifier\n",
    "* step 2: (line 6,7) load the image and convert it into gray-scale\n",
    "* step 3: (line 9) use detectMultiScale function of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "detectMultiScale(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]]) → objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Parameters \n",
    "\n",
    "* image – grayed scale image\n",
    "* scaleFactor – Parameter specifying how much the image size is reduced at each image scale.\n",
    "* minNeighbors – Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
    "\n",
    "\n",
    "* flags – Parameter with the same meaning for an old cascade as in the function cvHaarDetectObjects. It is not used for a new cascade.\n",
    "* minSize – Minimum possible object size. Objects smaller than that are ignored.\n",
    "* maxSize – Maximum possible object size. Objects larger than that are ignored.\n",
    "\n",
    "return\n",
    "\n",
    "* objects – Vector of rectangles where each rectangle contains the detected object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "for video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('pic/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Read the input image\n",
    "#img = cv2.imread('test.png')\n",
    "\n",
    "cap = cv2.VideoCapture('pic/face.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    _, img = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for (x, y , w ,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)\n",
    "\n",
    "    # Display the output\n",
    "    cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 36. Eye Detection Haar Feature based Cascade Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Trained Classifiers: https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "\n",
    "haarcascade_eye_tree_eyeglasses.xml 이미 수업폴더에 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('pic/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('pic/haarcascade_eye_tree_eyeglasses.xml')\n",
    "cap = cv2.VideoCapture('pic/head.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    _, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for (x, y , w ,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)\n",
    "        roi_gray = gray[y:y+h, x:x+w] #roi: region of interest\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex, ey ,ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex,ey), (ex+ew, ey+eh), (0, 255, 0), 5)\n",
    "\n",
    "    # Display the output\n",
    "    cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "* 얼굴을 detect 한 사각형의 위치를 범위로 주고 그 안에서 eye detect 를 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 37. Detect Corners with Harris Corner Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "디지털 이미지는 점들, 즉 픽셀들의 집합으로 구성되어 있습니다. 그 점들 중에서 가장 중요한 점들은 단연 코너점들입니다. 코너점(corner point)이란 두 방향 이상에서 변화가 급격한 점입니다. 반면 엣지점(edge point)은 한 방향에서 변화가 급격한 점입니다. 이도 저도 아닌 점들은 평탄한 점(flat point)이라고 부릅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/corner2.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "코너점이 중요한 이유는 이미지에서 가장 중요한 구조 정보를 담고 있기 때문입니다. 다른 정보를 모두 없애고 코너점들만 남겨놔도 이미지 내의 물체들의 형상을 대충 알 수 있습니다. \n",
    "\n",
    "가장 기초적이면서도 효율적인 코너점 검출기인 Harris 코너 검출기에 대해서 알아보자. Harris 코너 검출기는 1988년에 Harris와 Stephens에 의해 개발된 방법이다. \n",
    " \n",
    "Harris 코너 검출기의 기본 원리는 다음과 다.\n",
    "한 픽셀을 중심에 놓고 작은 윈도우(window)를 설정한 후에, 이 윈도우를 x축 방향으로 u만큼, y축 방향으로 v만큼 이동시킨다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/Harris.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "그 다음 윈도우 내의 픽셀 값들의 차이의 제곱의 합을 구해줍니다. \n",
    "\n",
    "한 마디로 얼마나 변화했는지를 계산해주는 것입니다. 코너점이라면 x축, y축 방향 모두 많이 변화했겠지요? 따라서, 이 E값이 크면 코너점이라고 보는 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Harris Corner Detector Algorithm**\n",
    "1. Determine which windows produce very large variations in intensity when moved in both X and Y directions.\n",
    "2. with each such window found, a score R is computed.\n",
    "3. After applying a threshold to this score, important corners are selected & marked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "\n",
    "img = cv2.imread('pic/chessboard2.png')\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "\n",
    "dst = cv2.dilate(dst, None)\n",
    "\n",
    "img[dst > 0.01 * dst.max()] = [0, 0, 255]\n",
    "\n",
    "cv2.imshow('dst', img)\n",
    "\n",
    "if cv2.waitKey(0) & 0xff == 27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "cv2.cornerHarris(src, blockSize, ksize, k[, dst[, borderType]]) → dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Parameters:\t\n",
    "* src – gray scale image.\n",
    "* blockSize – 코너검출에 이용할 픽셀의 크기\n",
    "* ksize – Sobel 미분에 사용되는 파라메터 값\n",
    "* k – Harris detector free parameter (코너검출 수학식에서). \n",
    "\n",
    "\n",
    "* borderType – Pixel extrapolation method. See borderInterpolate() .\n",
    "\n",
    "return\n",
    "* dst – Image to store the Harris detector responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "(line 12) 검출된 코너부분을 확대하기 위하여 dilate 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "(line 14) 원본 이미지에서 코너를 빨간색으로 표시. dst.max() 앞에 곱한 상수를 적절히 조절하면 검출된 코너를 최적화하여 화면에 표시 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 38. Detect Corners with Shi Tomasi Corner Detector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "better than Harr Corner detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Shi-Tomasi is almost similar to Harris Corner detector, apart from the way the score (R) is calculated. This gives a better result. Moreover, in this method, we can find the top N corners, which might be useful in case we don’t want to detect each and every corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('pic/pic1.png')\n",
    "\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "corners = cv.goodFeaturesToTrack(gray, 100, 0.01, 10)\n",
    "\n",
    "corners = np.int0(corners)\n",
    "\n",
    "for i in corners:\n",
    "    x, y = i.ravel()\n",
    "    cv.circle(img, (x, y), 3, [255, 255, 0], -1)\n",
    "\n",
    "cv.imshow('Shi-Tomasi Corner Detector', img)\n",
    "\n",
    "if cv.waitKey(0) & 0xff == 27:\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "cv2.goodFeaturesToTrack 함수는 4개의 인자를 받는다.\n",
    "* 첫번째는 특징점인 Corner를 추출할 입력 이미지로 1채널인 Grayscale 이미지를 받는다. \n",
    "* 두번째는 추출할 특징점의 개수이고 세번째는 특징점으로써의 최소 품질인데 품질의 범위는 0-1 사이값이다. 이 최소 품질보다 낮으면 제거한다. \n",
    "* 마지막 인자는 최소 거리로써 얻어진 특징점에서 이 최소 거리 이내의 특징점은 제거한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/s-1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/s-2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/s-3.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/s-4.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/s-5.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 39. How to Use Background Subtraction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('pic/vtest.avi')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "    cv.imshow('Frame', frame)\n",
    "\n",
    "    keyboard = cv.waitKey(30)\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "cap = cv2.VideoCapture('pic/vtest.avi')\n",
    "\n",
    "#fgbg = cv2.bgsegm.createBackgroundSubtractorMOG() # 이것이 더 좋으나, 현재 cv2 를 uninstall 하고 새로운 패키지 설치해야. 하지말것!!\n",
    "fgbg=cv2.createBackgroundSubtractorMOG2()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('FG MASK Frame', fgmask)\n",
    "\n",
    "    keyboard = cv2.waitKey(30)\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('pic/vtest.avi')\n",
    "#fgbg = cv.createBackgroundSubtractorMOG2(detectShadows=True)\n",
    "fgbg = cv.createBackgroundSubtractorKNN()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    \n",
    "    cv.imshow('Frame', frame)\n",
    "    cv.imshow('FG MASK Frame', fgmask)\n",
    "\n",
    "    keyboard = cv.waitKey(30)\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 40. Mean Shift Object Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "기본 아이디어는 아래 그림과 같이 추적하고자 하는 대상 물체에 대한 색상 히스토그램(histogram)과 현재 입력 영상의 히스토그램을 비교해서 가장 유사한 히스토그램을 갖는 윈도우 영역을 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/mean_shift.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "그런데 이렇게 직접 히스토그램을 비교할 경우에는 모든 가능한 윈도우 위치에 대해 각각 히스토그램을 구하고 또 비교해야 하기 때문에 시간이 너무 오래 걸린다. 실제로는 histogram backprojection 기법과 mean shift를 결합한 방법을 사용하는 것이 일반적이다. (이하 자세한 내용 생략)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('pic/slow_traffic_small.mp4')\n",
    "\n",
    "# step 1: take first frame of the video\n",
    "# step 2: setup initial location of window\n",
    "# step 3: set up the ROI for tracking\n",
    "# step 4: Setup the termination criteria, either 10 iteration or move by atleast 1 pt\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv.imshow('frame', frame)\n",
    "        k = cv.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "은색차의 전면 유리창을  tracking 해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('pic/slow_traffic_small.mp4')\n",
    "\n",
    "# step 1: take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "cv.imshow('frame', frame)\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        #cv.imshow('frame', frame)\n",
    "        k = cv.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('pic/slow_traffic_small.mp4')\n",
    "\n",
    "# step 1: take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# step 2: setup initial location of window\n",
    "x, y, width, height = 300, 200, 100, 50\n",
    "track_window = (x, y ,width, height)\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv.imshow('frame', frame)\n",
    "        k = cv.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('pic/slow_traffic_small.mp4')\n",
    "\n",
    "# step 1: take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# step 2: setup initial location of window\n",
    "x, y, width, height = 300, 200, 100, 50\n",
    "track_window = (x, y ,width, height)\n",
    "\n",
    "# step 3: set up the ROI for tracking\n",
    "roi = frame[y:y+height, x : x+width]\n",
    "hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
    "mask = cv.inRange(hsv_roi, np.array((0., 60., 32.)), np.array((180., 255., 255)))\n",
    "roi_hist = cv.calcHist([hsv_roi], [0], mask, [180], [0, 180])\n",
    "cv.normalize(roi_hist, roi_hist, 0, 255,cv.NORM_MINMAX)\n",
    "\n",
    "# step 4: Setup the termination criteria, either 10 iteration or move by atleast 1 pt\n",
    "term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1)\n",
    "cv.imshow('roi',roi)\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        dst = cv.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv.meanShift(dst, track_window, term_crit)\n",
    "        # Draw it on image\n",
    "        x,y,w,h = track_window\n",
    "        final_image = cv.rectangle(frame, (x,y), (x+w, y+h), 255, 3)\n",
    "\n",
    "        #cv.imshow('dst', dst)\n",
    "        cv.imshow('final_image',final_image)\n",
    "        k = cv.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]]) → hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Parameters:\t\n",
    "\n",
    "* images – Source arrays. They all should have the same depth, CV_8U or CV_32F , and the same size. \n",
    "* channels – List of the dims channels used to compute the histogram. \n",
    "* mask – Optional mask. If the matrix is not empty, it must be an 8-bit array of the same size as images[i]. \n",
    "* histSize – Array of histogram sizes in each dimension.\n",
    "* ranges – Array of the dims arrays of the histogram bin boundaries in each dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "return:\n",
    "\n",
    "* hist – Output histogram, which is a dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Mean Shift의 문제점\n",
    "1. size of window 가 변하지 않는다. (물체가 멀어지거나 다가 오는 경우에도)\n",
    "2. initial position을 주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Object Tracking Camshift Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "CamShift: Continuously Adoptive Mean Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "물체의 원근 및 회전에 따라 window size 및 rotation을 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "앞 장의 코드에서 line 29의 함수 이름만 meanShift-- > CamShift 바꾸자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('pic/slow_traffic_small.mp4')\n",
    "\n",
    "# step 1: take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# step 2: setup initial location of window\n",
    "x, y, width, height = 300, 200, 100, 50\n",
    "track_window = (x, y ,width, height)\n",
    "\n",
    "# step 3: set up the ROI for tracking\n",
    "roi = frame[y:y+height, x : x+width]\n",
    "hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
    "mask = cv.inRange(hsv_roi, np.array((0., 60., 32.)), np.array((180., 255., 255)))\n",
    "roi_hist = cv.calcHist([hsv_roi], [0], mask, [180], [0, 180])\n",
    "cv.normalize(roi_hist, roi_hist, 0, 255,cv.NORM_MINMAX)\n",
    "\n",
    "# step 4: Setup the termination criteria, either 10 iteration or move by atleast 1 pt\n",
    "term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1)\n",
    "cv.imshow('roi',roi)\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        dst = cv.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv.CamShift(dst, track_window, term_crit) #  meanShift-- > CamShift (파라메터도 동일)\n",
    "\n",
    "        # Draw it on image\n",
    "        x,y,w,h = track_window\n",
    "        final_image = cv.rectangle(frame, (x,y), (x+w, y+h), 255, 3)\n",
    "\n",
    "        #cv.imshow('dst', dst)\n",
    "        cv.imshow('final_image',final_image)\n",
    "        k = cv.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "차가 접근하에 따라 윈도우  크기가 커지는 것을 알 수 있다.  \n",
    "line 29의 ret 변수를 인쇄해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('pic/slow_traffic_small.mp4')\n",
    "\n",
    "# step 1: take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# step 2: setup initial location of window\n",
    "x, y, width, height = 300, 200, 100, 50\n",
    "track_window = (x, y ,width, height)\n",
    "\n",
    "# step 3: set up the ROI for tracking\n",
    "roi = frame[y:y+height, x : x+width]\n",
    "hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
    "mask = cv.inRange(hsv_roi, np.array((0., 60., 32.)), np.array((180., 255., 255)))\n",
    "roi_hist = cv.calcHist([hsv_roi], [0], mask, [180], [0, 180])\n",
    "cv.normalize(roi_hist, roi_hist, 0, 255,cv.NORM_MINMAX)\n",
    "\n",
    "# step 4: Setup the termination criteria, either 10 iteration or move by atleast 1 pt\n",
    "term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1)\n",
    "cv.imshow('roi',roi)\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        dst = cv.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv.CamShift(dst, track_window, term_crit) #  meanShift-- > CamShift (파라메터도 동일)\n",
    "        print(ret)\n",
    "        \n",
    "        # Draw it on image\n",
    "        x,y,w,h = track_window\n",
    "        final_image = cv.rectangle(frame, (x,y), (x+w, y+h), 255, 3)\n",
    "\n",
    "        #cv.imshow('dst', dst)\n",
    "        cv.imshow('final_image',final_image)\n",
    "        k = cv.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "((X좌표, Y좌표),(height, width, rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "이 return 값을 모두 활용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('pic/slow_traffic_small.mp4')\n",
    "# take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "# setup initial location of window\n",
    "x, y, width, height = 300, 200, 100, 50\n",
    "track_window = (x, y ,width, height)\n",
    "# set up the ROI for tracking\n",
    "roi = frame[y:y+height, x : x+width]\n",
    "hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
    "mask = cv.inRange(hsv_roi, np.array((0., 60., 32.)), np.array((180., 255., 255)))\n",
    "roi_hist = cv.calcHist([hsv_roi], [0], mask, [180], [0, 180])\n",
    "cv.normalize(roi_hist, roi_hist, 0, 255,cv.NORM_MINMAX)\n",
    "# Setup the termination criteria, either 10 iteration or move by atleast 1 pt\n",
    "term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1)\n",
    "cv.imshow('roi',roi)\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        dst = cv.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv.CamShift(dst, track_window, term_crit)  #  meanShift-- > CamShift. (파라메터도 동일)\n",
    "\n",
    "        # Draw it on image\n",
    "        pts = cv.boxPoints(ret)  # floating value 를 return\n",
    "        print(pts)\n",
    "        pts = np.int0(pts) # integer로 변환\n",
    "        final_image = cv.polylines(frame, [pts], True, (0, 255, 0), 2)\n",
    "        #x,y,w,h = track_window\n",
    "        #final_image = cv.rectangle(frame, (x,y), (x+w, y+h), 255, 3)\n",
    "\n",
    "        #cv.imshow('dst', dst)\n",
    "        cv.imshow('final_image',final_image)\n",
    "        k = cv.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Face Detection: Opencv vs Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# plot photo with detected faces using opencv cascade classifier\n",
    "from cv2 import imread\n",
    "from cv2 import imshow\n",
    "from cv2 import waitKey\n",
    "from cv2 import destroyAllWindows\n",
    "from cv2 import CascadeClassifier\n",
    "from cv2 import rectangle\n",
    "# load the photograph\n",
    "pixels = imread('pic/test1.jpg')\n",
    "\n",
    "imshow('face detection', pixels)\n",
    "# keep the window open until we press a key\n",
    "waitKey(0)\n",
    "# close the window\n",
    "destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load the pre-trained model\n",
    "classifier = CascadeClassifier('pic/haarcascade_frontalface_default.xml')\n",
    "# perform face detection\n",
    "bboxes = classifier.detectMultiScale(pixels)\n",
    "# print bounding box for each detected face\n",
    "for box in bboxes:\n",
    "    # extract\n",
    "    x, y, width, height = box\n",
    "    x2, y2 = x + width, y + height\n",
    "    # draw a rectangle over the pixels\n",
    "    rectangle(pixels, (x, y), (x2, y2), (0,0,255), 1)\n",
    "# show the image\n",
    "imshow('face detection', pixels)\n",
    "# keep the window open until we press a key\n",
    "waitKey(0)\n",
    "# close the window\n",
    "destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load the photograph\n",
    "pixels = imread('pic/test2.jpg')\n",
    "\n",
    "imshow('face detection', pixels)\n",
    "# keep the window open until we press a key\n",
    "waitKey(0)\n",
    "# close the window\n",
    "destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load the pre-trained model\n",
    "classifier = CascadeClassifier('pic/haarcascade_frontalface_default.xml')\n",
    "# perform face detection\n",
    "bboxes = classifier.detectMultiScale(pixels)\n",
    "# print bounding box for each detected face\n",
    "for box in bboxes:\n",
    "    # extract\n",
    "    x, y, width, height = box\n",
    "    x2, y2 = x + width, y + height\n",
    "    # draw a rectangle over the pixels\n",
    "    rectangle(pixels, (x, y), (x2, y2), (0,0,255), 1)\n",
    "# show the image\n",
    "imshow('face detection', pixels)\n",
    "# keep the window open until we press a key\n",
    "waitKey(0)\n",
    "# close the window\n",
    "destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load the photograph\n",
    "pixels = imread('pic/test2.jpg')\n",
    "\n",
    "imshow('face detection', pixels)\n",
    "# keep the window open until we press a key\n",
    "waitKey(0)\n",
    "# close the window\n",
    "destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load the pre-trained model\n",
    "classifier = CascadeClassifier('pic/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# perform face detection\n",
    "###########################################\n",
    "bboxes = classifier.detectMultiScale(pixels, 1.05, 8)\n",
    "\n",
    "# print bounding box for each detected face\n",
    "for box in bboxes:\n",
    "    # extract\n",
    "    x, y, width, height = box\n",
    "    x2, y2 = x + width, y + height\n",
    "    # draw a rectangle over the pixels\n",
    "    rectangle(pixels, (x, y), (x2, y2), (0,0,255), 1)\n",
    "# show the image\n",
    "imshow('face detection', pixels)\n",
    "# keep the window open until we press a key\n",
    "waitKey(0)\n",
    "# close the window\n",
    "destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
